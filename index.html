<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Attribution-Guided Machine Unlearning </title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1 {
            color: #2c3e50;
        }
        h2 {
            color: #34495e;
        }
        .authors {
            font-style: italic;
            color: #7f8c8d;
        }
        .abstract {
            background-color: #f2f2f2;
            padding: 15px;
            border-radius: 5px;
        }
        figure {
            text-align: center;
        }
        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <header>
        <h1>
            Machine Unlearning via Simulated Oracle Matching
            <br>
            <em>[website in progress!]</em>
        </h1> 
        <p class="authors">Authors:
                
            <a href="https://kristian-georgiev.github.io">Kristian Georgiev</a>, <a href="http://sungminpark.com">Sung Min Park</a>,
            <a href="https://royrinberg.com">Roy Rinberg</a>,
            <br>
             <a href="https://cs.stanford.edu/people/shivamg/">Shivam Garg</a>, <a href="https://andrewilyas.com">Andrew Ilyas</a>, <a href="http://madry.mit.edu">Aleksander Madry</a>, <a href="https://sethneel.com">Seth Neel</a>

        </p>

        <!--         <p class="authors">Authors: Kristian Georgiev, Sung Min Park, Roy Rinberg, Shivam Garg, Andrew Ilyas, Aleksander Madry, Seth Neel            
        </p>
        -->
    </header>

    <a href="https://royrinberg.com//assets/projects/papers/Unlearning_through_Data_Attribution.pdf" style="font-weight: bold; font-size: 1.2em;">Link to 2 page Abstract</a>

    <section class="abstract">
        <h2>Abstract</h2>
        <p>
            Despite increasing interest in machine unlearning, recent work shows that under strong evaluations,
            existing techniques largely fail to unlearn in non-convex settings. In this paper, we introduce a new
            technique for machine unlearning in such settings. Key to our method is a reduction from the problem of
            machine unlearning to that of data attribution. In particular, we show theoretically (in an underdetermined
            regression setting) and empirically (in a standard deep learning setting) that given access to the outputs of a
            perfectly unlearned model (i.e., a model trained from scratch on the non-unlearned data), we can quickly
            fine-tune an existing model on these predictions and match the target model predictions out-of-sample.
            Meanwhile, predicting such “oracle” outputs is precisely the goal of a recent line of work in data attribution
            called datamodeling. Combining these two insights yields an end-to-end unlearning algorithm in which one
            first predicts the output of a model re-trained from scratch, then fine-tunes an existing model to match these
            predicted outputs. Across different types and sizes of forget sets on standard classification tasks, we show
            that this two-stage algorithm results in strong unlearning performance being close to indistinguishable
            from the fully-retrained “oracle” model in some cases. As an added benefit, our reduction means that future
            improvements to data attribution—whether in accuracy or efficiency—may in turn yield better unlearning
            algorithms
        </p>
    </section>


    <!-- INTRODUCTION -->


    <section>
        <h2>Introduction</h2> 
        <!-- How to do citation: <sup><a href="#ref1">2</a></sup> -->

        <p>The goal of machine <em>unlearning</em> is to remove (or "unlearn") the impact of a specific collection of training examples from a trained machine learning model. Recent interest in machine unlearning arguably began with privacy-focused initiatives such as the GDPR's <em>Right to be Forgotten</em>, which asks companies to comply with "data deletion requests" from their users 
            <sup><a href="#ginart2019making">cite</a> </sup>
            --- upon receiving a request, the company's goal is to change their AI models to be as if they had never seen the relevant user's data. Since GDPR, unlearning has found a variety of applications outside of privacy. These include, for example, removing the effect of toxic, outdated, or poisoned data 
            <sup><a href="#pawelczyk2024machineunlearningfailsremove">cite</a>  </sup> 
            <sup><a href="#goel2024correctivemachineunlearning">cite</a> </sup>,; rectifying copyright infringement 
            <sup><a href="#liu2024unlearning">cite</a>  </sup> 
            ; or aligning language models 
            <sup><a href="#li2024wmdpbenchmarkmeasuringreducing">cite</a>  </sup>.</p>

        <p>The relevance of machine unlearning has prompted a growing line of research on technical solutions in the form of <em>unlearning algorithms</em>. An unlearning algorithm takes as input a model &theta; trained on a dataset S and a "forget set" S<sub>F</sub> &sub; S, and outputs a model &theta;' that "looks like" it was trained on the so-called "retain set" S<sub>R</sub> := S &minus; S<sub>F</sub>. Of course, one valid unlearning algorithm simply throws away the original model &theta;, and retrains a new model &theta;' from scratch on the retain set S<sub>R</sub>. This algorithm clearly succeeds at the task of unlearning, since the generated &theta;' really <em>is</em> trained on the retain set. As model and dataset sizes continue to increase, however, this approach becomes an increasingly costly proposition. Thus, most work on unlearning seeks to approximate this naive algorithm while imposing a lower computational burden.</p>
    
        <h2>Existing Data</h2> 

<!--
        <p><strong>Current approaches to unlearning.</strong> For simple (e.g., convex or linear) models, there are fast unlearning algorithms that also enjoy provable guarantees. These methods fall into two classes: fine-tuning the existing model using gradient-based approaches <cite>kurmanji2023towards, neel2021deletion, graves2021amnesiac</cite>, or exploiting specific properties of the model <cite>approximate_data_deletion, mahadevan2021certifiablemachineunlearninglinear, suriyakumar2022algorithms</cite>. In larger neural networks (where efficient unlearning is arguably most relevant, given the cost of training from scratch), the situation is considerably murkier. The only methods that obtain provable guarantees tend to come with significant degradation in accuracy <cite>sisa, li2022largelanguagemodelsstrong</cite>.</p>

        <p>As a result, most unlearning algorithms directly apply fine-tuning based methods to non-convex models, and then evaluate the resulting models empirically. In particular, such algorithms tend to start from a trained model &theta;, and then use a combination of (a) gradient <em>descent</em> to minimize loss on the retain set S<sub>R</sub> <cite>neel2021deletion</cite>; and (b) gradient <em>ascent</em> to maximize loss on the forget set S<sub>F</sub> <cite>jang2022knowledge</cite>. These two techniques are combined with additional heuristics like regularizing towards &theta;, adding noise during optimization, freezing some parameters <cite>fan2024salunempoweringmachineunlearning</cite>, or adding sparsity <cite>kurmanji2023towards, jia2024modelsparsitysimplifymachine</cite>.</p>

        <p><strong>A recurring issue with current methods.</strong> <cite>hayes2024inexact</cite> recently performed a fine-grained analysis of gradient ascent/descent-based unlearning methods on CIFAR-10. They find that, when subjected to strong evaluations <cite>kurmanji2023towards, pawelczyk2023incontext</cite>, existing methods fail to unlearn, and in fact <em>reduce</em> the privacy protection for some examples after unlearning (Figure 3 in <cite>hayes2024inexact</cite>, which we independently confirm in our Figure 1).</p>

        <p>One intuitive reason for this failure suggested by <cite>hayes2024inexact</cite> is <em>stopping time</em> error. With gradient ascent, for example, one can make the loss on the forget set arbitrarily high---but the goal is <em>not</em> a model with infinite loss on the forget set. Instead, we want the unlearned model to have the same loss as a model trained only on the retain set; and in fact, we want this similarity to hold <em>pointwise</em> over the points in the forget set. This introduces two immediate problems. First, we lack access to the "target losses" we are trying to match, and thus do not know when to stop taking gradient ascent steps. Even worse, different examples may have target losses that require a different number of steps to reach, leading some examples being "overshot" and others being "undershot." The story is similar for gradient descent on the retain set S<sub>R</sub>, with some added complexity coming from the fact that modern ML settings are overparameterized, meaning that (a) the original model &theta; may already have zero loss on the retain set; and (b) retain set performance is insufficient to specify forget set performance, so the model will continue to change its performance on the forget set even after interpolating S<sub>R</sub>. (We note that these challenges disappear entirely in the strongly convex case---there, gradient descent on the retain set converges to the unique minimizer of the retain set loss, which by definition is the target model.)</p>

        <p><strong>This work.</strong> In this paper, we present a new unlearning algorithm that side-steps the stopping time issue discussed above, and (empirically) achieves state-of-the-art unlearning performance. Our algorithm resembles prior techniques in that we rely on fine-tuning the original model &theta;. We deviate from prior work, however, through two main ideas:</p>
        <ol>
            <li><strong>Oracle matching.</strong> We begin with a thought experiment: what if we could access the <em>outputs</em> (but not the parameters) of a model trained on the retain set S<sub>R</sub>? We show that such "oracle" access directly enables an efficient, fine tuning-based unlearning algorithm. Rather than minimizing/maximizing loss on the retain/forget sets, this algorithm simply picks a subset of the entire dataset that includes the forget set, and minimizes the difference between model outputs and outputs from the oracle. Conceptually, this algorithm is "stable" in that upon convergence, the model agrees with the oracle on all the fine-tuning points, including those in the forget set---thus, we do not need to carefully set a stopping time. Empirically, we find that the fine-tuned model also <em>generalizes</em> beyond the fine-tuning points, and in some way "distills" the target model effectively into parameters &theta;'.</li>
            <li><strong>Oracle simulation.</strong> Of course, the method we just described on its own is not an unlearning algorithm---it relies on oracle access to the very model that it aims to replicate. It does, however, reduce the task of designing an unlearning algorithm to the task of constructing such an oracle. And while this task may at first seem just as difficult as unlearning itself, it turns out to be an independent subject of study in the field of <em>data attribution</em>. In particular, the <em>datamodeling</em> framework <cite>datamodels</cite> is precisely concerned with the task of predicting the output of a machine learning model as a direct function of its training data. This leads us to our second idea: instead of using "oracle" outputs from a re-trained oracle, why not just <em>simulate</em> these outputs using known data attribution techniques? We show that despite these techniques being imperfect, applying our oracle matching algorithm to <em>predicted</em> oracle outputs works nearly as well as using the true oracle outputs.</li>
        </ol>
        <p>The resulting <em>DM-Matching</em> algorithm not only achieves current state-of-the-art performance, but also introduces a reduction from unlearning to <em>datamodeling</em>-style data attribution, allowing us to translate future improvements in the latter field to better algorithms for the former.</p>
        <figure>
            <img src="assets/figs/headline_plot_pareto__99__20240711-0152.pdf" alt="Data attribution based unlearning algorithms are highly effective relative to oracle re-training" style="width:99%;">
            <figcaption><strong>Data attribution based unlearning algorithms are highly effective relative to oracle re-training</strong>. We evaluate the unlearning effectiveness of different approximate unlearning methods using the KLOM metric (y-axis), which measures the distributional difference between unlearned predictions and oracle predictions (0 being perfect). To contextualize each method's efficiency, we also show the amount of compute relative to full re-training (x-axis). Here, we show results on two challenging (e.g., small or non-random) forget sets on CIFAR-10. The KLOM values are evaluated over points in the forget set, retain set, and validation set to ensure that unlearning is effective across all datapoints; we also report their average. Overall, datamodel-based methods (DM-Predict and DM-Matching) dominate the pareto frontier of existing unlearning methods, and approach the unlearning quality of oracle models (full re-training) at a much smaller fraction of the cost.</figcaption>
        </figure>

        <p><strong>Contributions and roadmap.</strong> Concretely, our main contributions are as follows:</p>
        <ol>
            <li>In Section 2, we propose a new evaluation metric for unlearning, called <em>KL divergence of Margins</em> (KLOM). KLOM directly adapts a formal definition of unlearning <cite>neel2021deletion</cite> to be computationally and statistically tractable to estimate; we discuss how KLOM addresses some challenges with existing unlearning metrics.</li>
            <li>In Section 3, we introduce the idea of oracle matching (OM) from Step 1 above, where one fine-tunes an original model to minimize distance between model outputs and oracle outputs (again, assuming one has access to such outputs). Empirically, oracle matching seems to effectively distill the oracle model in a fraction of the time it takes to re-train. Consequently, OM yields a state-of-the-art "unlearning method," both in terms of the strongest existing evaluation (ULIRA) and our proposed KLOM metric.</li>
            <li>In Section 4, we provide some theoretical justification for oracle matching using a case study of underdetermined ridge regression. Our main result is that, when the regularization constant is not too large, oracle matching converges to the target model strictly faster than gradient descent on the retain set, while gradient ascent-descent may fail to converge at all.</li>
            <li>In Section 5, we finally turn oracle matching into a valid unlearning algorithm using the "oracle simulation" idea discussed in Step 2 above. We show using both KLOM and ULIRA evaluations that our final <em>DM-Matching</em> algorithm Pareto-dominates existing unlearning algorithms in terms of accuracy and efficiency, and is in fact competitive with from-scratch re-training while being a fraction of the cost. (Figure 1 illustrates this for KLOM.)</li>
        </ol>
        <p>We conclude with a discussion of limitations, possible extensions, and future work.</p>
    </section>
-->

    <!-- Preliminaries-->

    <!-- MEASURES OF UNLEARNING-->


    <section>
        <h2>An Improved Evaluation Metric: KLOM</h2>
        <p>In this section, we propose the <em>KL divergence of margins</em> or <strong>KLOM</strong> as an improved way to empirically measure unlearning. 
        <strong>KLOM</strong> improves over <strong>ULIRA</strong> in three key ways: (i) it is conceptually simpler in that it directly approximates Definition 1 (ii) it is simpler to implement resulting in more consistency across papers and (iii) most importantly, it is not susceptible to gaming by "catastrophic unlearning," as we discuss below.</p>

        <p>Recall from the previous section that traditionally the target of unlearning algorithms has been 
        <strong>(&#949;, &#948;)</strong>-approximate unlearning (Definition 1).
        The intractability of evaluating this definition directly led to the development of <strong>ULIRA</strong>, which evaluates unlearning by comparing distributions of model outputs <strong>f<sub>x</sub></strong> rather than distributions of models. Now we note that Definition 1 can be written as 
        </p>
        <p style="text-align: center;">
            &#916; (<strong>&#120592;</strong>(<strong>&#120592;</strong>(<strong>D</strong>), <strong>S<sub>F</sub></strong>), <strong>safe</strong>(<strong>S<sub>F</sub></strong>)) &#8804; &#949;,
        </p>
        <p>where &#916; denotes the &#948;-approximate max-KL divergence, and <strong>safe</strong>(<strong>S<sub>F</sub></strong>) denotes the re-training distribution <strong>&#120592;</strong>(<strong>S<sub>R</sub></strong>). While the approximate max-KL was chosen due to its appealing privacy properties, particularly when evaluating empirical unlearning algorithms it is sensible to consider other divergences that can be easily evaluated empirically. We also note that while <strong>safe</strong>(<strong>S<sub>F</sub></strong>) was chosen as the re-training distribution since it does not depend on <strong>S<sub>F</sub></strong> and also achieves high accuracy, from an unlearning perspective any <strong>safe</strong>(<strong>S<sub>F</sub></strong>) distribution that doesn't depend on <strong>S<sub>F</sub></strong> would constitute a valid unlearning target. For example, any distribution on models expressible as a function of a collection of retrained models (such as one that averages the logits output by 10 retrained model) would be a valid target distribution here. We mention this here, because as we will see in the subsequent sections, using techniques from data attribution predictions from such an ensemble model may be easier to approximate than a retrained model.</p>

        <p>We now define <strong>KLOM</strong>, which corresponds to Definition 1 where we use &#916; = KL divergence rather than &#948;-approximate max-KL, and as in <strong>ULIRA</strong>, we compute the divergence between distributions of model outputs <strong>f<sub>x</sub></strong> rather than models.</p>

        <h3>Definition: KL divergence of margins (KLOM)</h3>
        <p>For an unlearning algorithm <strong>&#120592;</strong> and fixed input <strong>x</strong>, 
        the KL divergence of margins is given by
        </p>
        <p style="text-align: center;">
            <strong>KLOM</strong>(<strong>&#120592;</strong>) := D<sub>KL</sub>(<strong>f<sub>x</sub>(&#120592;</strong>(<strong>S</strong> &#8722; <strong>S<sub>F</sub></strong>), <strong>f<sub>x</sub>(&#120592;</strong>(<strong>&#120592;</strong>(<strong>S</strong>), <strong>S<sub>F</sub></strong>))) &#8804; &#949;,
        </p>
        <p>Throughout the paper, we primarily evaluate unlearning algorithms via computing the <strong>KLOM</strong> for different inputs <strong>x &#8712; S<sub>f</sub>, S<sub>r</sub></strong>, or test points <strong>x &#8764; &#120121;</strong>. We also evaluate our algorithms with <strong>ULIRA</strong>, and defer these results to the Appendix. Finally, we comment on the major shortcoming of <strong>ULIRA</strong>.</p>

        <h3>Gaming ULIRA with Catastrophic Unlearning</h3>
        <p>We note that <strong>ULIRA</strong> measures the ability of an adversary to distinguish unlearned model margins on test points, from unlearned model margins on train points that were subsequently unlearned. The issue occurs because regardless of if a point is sampled from the forget set or is a test point, the margin is still computed with respect to the unlearned model. So consider an unlearning algorithm that simply outputs a constant prediction on every input subsequent to unlearning. This is a terrible unlearning algorithm when evaluated via Definition 1 since it is maximally far from the re-training distribution, but at the same time it will achieve a perfect <strong>ULIRA</strong> score of 50%, since the distribution of forget and test point margins will both be the same (constant). Note that the <strong>KLOM</strong> score for this model will still be very large, since <strong>KLOM</strong> directly compute the KL divergence to the re-trained model margin. We explain this further in Appendix and provide empirical evidence in the Figure below.</p>


        <p>We include a visual representation of our algorithm below.</p>
        <figure>
            <img src="assets/figs/Measures_of_Unlearning.pdf" alt="Visual diagram of KLOM" style="width:100%;">
            <figcaption>Visual diagram of KLOM</figcaption>
        </figure>
        
    </section>


    <!-- Oracle Matching -->
    <!-- Datamodels Matching -->


    <!-- <main>
        <section>
            <h2>1. Introduction</h2>
            <p>
                Initially motivated by privacy considerations such as the GDPR's "Right to be Forgotten," machine unlearning seeks to address the challenge of removing the influence of (or "forgetting") selected datapoints from an existing model.
            </p>
            <figure>
                <img src="assets/figs/9__headline_plot_pareto__99__20240521-1710.pdf" alt="High-level description of Machine Unlearning">
                <figcaption>Figure 1: High-level description of Machine Unlearning.</figcaption>
            </figure>
        </section>

        <section>
            <h2>2. Unlearning through Data Attribution</h2>
            <p>
                We seek to side-step the problems that arise in the prior heuristic-based machine unlearning methods, by investigating what an optimal unlearner would do given access to predictions of a model retrained from scratch on the retain set.
            </p>
            <figure>
                <img src="path/to/your/figure2.png" alt="Data attribution based unlearning methods">
                <figcaption>Figure 2: Data attribution based unlearning methods significantly outperform prior unlearning methods.</figcaption>
            </figure>
        </section>

        <section>
            <h2>3. Conclusion and discussion</h2>
            <p>
                We show that leveraging data attribution methods offers a fruitful direction for machine unlearning. By leveraging estimates of predictions of the (counterfactual) retrained model, we can sidestep the issue of unknown stopping conditions of prior gradient-based approaches.
            </p>
        </section>
        
    -->
        <section>
            <h2>References</h2>
            <ol>
                <li id="suriyakumar2022algorithms">Suriyakumar, Vinith and Wilson, Ashia C. "Algorithms that approximate data removal: New results and limitations." <em>Advances in Neural Information Processing Systems</em>, vol. 35, 2022, pp. 18892–18903.</li>
                <li id="giordano2023bayesian">Giordano, Ryan and Broderick, Tamara. "The Bayesian Infinitesimal Jackknife for Variance." <em>arXiv preprint arXiv:2305.06466</em>, 2023.</li>
                <li id="rad2018scalable">Rad, Kamiar Rahnama and Maleki, Arian. "A scalable estimate of the extra-sample prediction error via approximate leave-one-out." <em>arXiv preprint arXiv:1801.10243</em>, 2018.</li>
                <li id="wilson2020approximate">Wilson, Ashia, Kasy, Maximilian, and Mackey, Lester. "Approximate cross-validation: Guarantees for model assessment and selection." In: <em>International conference on artificial intelligence and statistics</em>, PMLR, 2020, pp. 4530–4540.</li>
                <li id="stephenson2020approximate">Stephenson, William and Broderick, Tamara. "Approximate cross-validation in high dimensions with guarantees." In: <em>International conference on artificial intelligence and statistics</em>, PMLR, 2020, pp. 2424–2434.</li>
                <li id="jia2024modelsparsitysimplifymachine">Jia, Jinghan, Liu, Jiancheng, Ram, Parikshit, Yao, Yuguang, Liu, Gaowen, Liu, Yang, Sharma, Pranay, and Liu, Sijia. "Model Sparsity Can Simplify Machine Unlearning." <em>arXiv preprint arXiv:2304.04934</em>, 2024. <a href="https://arxiv.org/abs/2304.04934" target="_blank">https://arxiv.org/abs/2304.04934</a></li>
                <li id="feldmanmemorization">Feldman, Vitaly. "Does Learning Require Memorization? A Short Tale about a Long Tail." <em>arXiv preprint arXiv:1906.05271</em>, 2021. <a href="https://arxiv.org/abs/1906.05271" target="_blank">https://arxiv.org/abs/1906.05271</a></li>
                <li id="fan2024salunempoweringmachineunlearning">Fan, Chongyu, Liu, Jiancheng, Zhang, Yihua, Wong, Eric, Wei, Dennis, and Liu, Sijia. "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation." <em>arXiv preprint arXiv:2310.12508</em>, 2024. <a href="https://arxiv.org/abs/2310.12508" target="_blank">https://arxiv.org/abs/2310.12508</a></li>
                <li id="mahadevan2021certifiablemachineunlearninglinear">Mahadevan, Ananth and Mathioudakis, Michael. "Certifiable Machine Unlearning for Linear Models." <em>arXiv preprint arXiv:2106.15093</em>, 2021. <a href="https://arxiv.org/abs/2106.15093" target="_blank">https://arxiv.org/abs/2106.15093</a></li>
                <li id="sisa">Bourtoule, Lucas, Chandrasekaran, Varun, Choquette-Choo, Christopher A., Jia, Hengrui, Travers, Adelin, Zhang, Baiwu, Lie, David, and Papernot, Nicolas. "Machine Unlearning." <em>arXiv preprint arXiv:1912.03817</em>, 2020. <a href="https://arxiv.org/abs/1912.03817" target="_blank">https://arxiv.org/abs/1912.03817</a></li>
                <li id="li2022largelanguagemodelsstrong">Li, Xuechen, Tramèr, Florian, Liang, Percy, and Hashimoto, Tatsunori. "Large Language Models Can Be Strong Differentially Private Learners." <em>arXiv preprint arXiv:2110.05679</em>, 2022. <a href="https://arxiv.org/abs/2110.05679" target="_blank">https://arxiv.org/abs/2110.05679</a></li>
                <li id="grosse2023studying">Grosse, Roger, Bae, Juhan, Anil, Cem, Elhage, Nelson, Tamkin, Alex, Tajdini, Amirhossein, Steiner, Benoit, Li, Dustin, Durmus, Esin, Perez, Ethan, et al. "Studying large language model generalization with influence functions." <em>arXiv preprint arXiv:2308.03296</em>, 2023.</li>
                <li id="santurkar2020breeds">Santurkar, Shibani, Tsipras, Dimitris, and Madry, Aleksander. "BREEDS: Benchmarks for Subpopulation Shift." <em>ArXiv preprint arXiv:2008.04859</em>, 2020.</li>
                <li id="pawelczyk2024machineunlearningfailsremove">Pawelczyk, Martin, Di, Jimmy Z., Lu, Yiwei, Kamath, Gautam, Sekhari, Ayush, and Neel, Seth. "Machine Unlearning Fails to Remove Data Poisoning Attacks." <em>arXiv preprint arXiv:2406.17216</em>, 2024. <a href="https://arxiv.org/abs/2406.17216" target="_blank">https://arxiv.org/abs/2406.17216</a></li>
                <li id="goel2023adversarialevaluationsinexactmachine">Goel, Shashwat, Prabhu, Ameya, and Sanyal, Amartya. "Towards Adversarial Evaluations for Inexact Machine Unlearning." <em>arXiv preprint arXiv:2201.06640</em>, 2023. <a href="https://arxiv.org/abs/2201.06640" target="_blank">https://arxiv.org/abs/2201.06640</a></li>
                <li id="vyas2023provablecopyrightprotectiongenerative">Vyas, Nikhil, Kakade, Sham, and Barak, Boaz. "On Provable Copyright Protection for Generative Models." <em>arXiv preprint arXiv:2302.10870</em>, 2023. <a href="https://arxiv.org/abs/2302.10870" target="_blank">https://arxiv.org/abs/2302.10870</a></li>
                <li id="koh2017understanding">Koh, Pang Wei and Liang, Percy. "Understanding black-box predictions via influence functions." In: <em>International conference on machine learning</em>, PMLR, 2017, pp. 1885–1894.</li>
                <li id="mu_survey">Nguyen, Thanh Tam, Huynh, Thanh Trung, Nguyen, Phi Le, Liew, Alan Wee-Chung, Yin, Hongzhi, and Nguyen, Quoc Viet Hung. "A Survey of Machine Unlearning." <em>arXiv preprint arXiv:2209.02299</em>, 2022.</li>
                <li id="neurips2023unlearning">Triantafillou, Eleni, Pedregosa, Fabian, Guyon, Isabelle, Escalera, Sergio, Jacques Junior, Julio C. S., Dziugaite, Gintare Karolina, Triantafillou, Peter, Dumoulin, Vincent, Mitliagkas, Ioannis, Sun Hosoya, Lisheng, Kurmanji, Meghdad, Zhao, Kairan, Wan, Jun, Kairouz, Peter. "NeurIPS 2023 Machine Unlearning Challenge." <a href="https://unlearning-challenge.github.io" target="_blank">https://unlearning-challenge.github.io</a>. Accessed: 2024-05-29.</li>
                <li id="engstrom2024dsdm">Engstrom, Logan, Feldmann, Axel, and Madry, Aleksander. "DsDm: Model-Aware Dataset Selection with Datamodels." <em>arXiv preprint arXiv:2401.12926</em>, 2024.</li>
                <li id="pawelczyk2023incontext">Pawelczyk, Martin, Neel, Seth, and Lakkaraju, Himabindu. "In-Context Unlearning: Language Models as Few Shot Unlearners." <em>arXiv preprint arXiv:2310.07579</em>, 2023.</li>
                <li id="hayes2024inexact">Hayes, Jamie, Shumailov, Ilia, Triantafillou, Eleni, Khalifa, Amr, and Papernot, Nicolas. "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy." <em>arXiv preprint arXiv:2403.01218</em>, 2024.</li>
                <li id="liu2024unlearning">Liu, Ken Ziyu. "Machine unlearning in 2024." Ken Ziyu Liu - Stanford Computer Science, Apr. 2024. <a href="https://ai.stanford.edu/~kzliu/blog/unlearning" target="_blank">https://ai.stanford.edu/~kzliu/blog/unlearning</a></li>
                <li id="henderson2023foundation">Henderson, Peter, Li, Xuechen, Jurafsky, Dan, Hashimoto, Tatsunori, Lemley, Mark A., and Liang, Percy. "Foundation Models and Fair Use." <em>arXiv preprint arXiv:2303.15715</em>, 2023.</li>
                <li id="nguyen">Nguyen, Thanh Tam, Huynh, Thanh Trung, Nguyen, Phi Le, Liew, Alan Wee-Chung, Yin, Hongzhi, and Nguyen, Quoc Viet Hung. "A Survey of Machine Unlearning." <em>arXiv preprint arXiv:2209.02299</em>, 2022.</li>
                <li id="borisov2022language">Borisov, Vadim, Seßler, Kathrin, Leemann, Tobias, Pawelczyk, Martin, and Kasneci, Gjergji. "Language models are realistic tabular data generators." In: <em>International Conference on Learning Representations (ICLR)</em>, 2023.</li>
                <li id="radford2019language">Radford, Alec, Wu, Jeffrey, Child, Rewon, Luan, David, Amodei, Dario, Sutskever, Ilya, et al. "Language models are unsupervised multitask learners." <em>OpenAI blog</em>, vol. 1, no. 8, 2019, pp. 9.</li>
                <li id="bubeck2023sparks">Bubeck, Sébastien, Chandrasekaran, Varun, Eldan, Ronen, Gehrke, Johannes, Horvitz, Eric, Kamar, Ece, Lee, Peter, Lee, Yin Tat, Li, Yuanzhi, Lundberg, Scott, et al. "Sparks of artificial general intelligence: Early experiments with gpt-4." <em>arXiv preprint arXiv:2303.12712</em>, 2023.</li>
                <li id="wei2021finetuned">Wei, Jason, Bosma, Maarten, Zhao, Vincent Y., Guu, Kelvin, Yu, Adams Wei, Lester, Brian, Du, Nan, Dai, Andrew M., and Le, Quoc V. "Finetuned language models are zero-shot learners." <em>arXiv preprint arXiv:2109.01652</em>, 2021.</li>
                <li id="schick2020s">Schick, Timo and Schütze, Hinrich. "It's not just size that matters: Small language models are also few-shot learners." <em>arXiv preprint arXiv:2009.07118</em>, 2020.</li>
                <li id="kojima2022large">Kojima, Takeshi, Gu, Shixiang Shane, Reid, Machel, Matsuo, Yutaka, and Iwasawa, Yusuke. "Large language models are zero-shot reasoners." <em>Advances in neural information processing systems</em>, 2022.</li>
                <li id="brown2020language">Brown, Tom, Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared D., Dhariwal, Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, et al. "Language models are few-shot learners." <em>Advances in neural information processing systems</em>, vol. 33, 2020, pp. 1877–1901.</li>
                <li id="hiddenpoison">Di, Jimmy Z., Douglas, Jack, Acharya, Jayadev, Kamath, Gautam, and Sekhari, Ayush. "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks." <em>arXiv preprint arXiv:2212.10717</em>, 2022.</li>
                <li id="privacy_onion">Carlini, Nicholas, Jagielski, Matthew, Zhang, Chiyuan, Papernot, Nicolas, Terzis, Andreas, and Tramer, Florian. "The Privacy Onion Effect: Memorization is Relative." <em>arXiv preprint arXiv:2206.10469</em>, 2022.</li>
                <li id="goel2023adversarial">Goel, Shashwat, Prabhu, Ameya, Sanyal, Amartya, Lim, Ser-Nam, Torr, Philip, and Kumaraguru, Ponnurangam. "Towards Adversarial Evaluations for Inexact Machine Unlearning." <em>arXiv preprint arXiv:2201.06640</em>, 2023.</li>
                <li id="min2022rethinking">Min, Sewon, Lyu, Xinxi, Holtzman, Ari, Artetxe, Mikel, Lewis, Mike, Hajishirzi, Hannaneh, and Zettlemoyer, Luke. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?" In: <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 11048–11064.</li>
                <li id="wei2023larger">Wei, Jerry, Wei, Jason, Tay, Yi, Tran, Dustin, Webson, Albert, Lu, Yifeng, Chen, Xinyun, Liu, Hanxiao, Huang, Da, Zhou, Denny, et al. "Larger language models do in-context learning differently." <em>arXiv preprint arXiv:2303.03846</em>, 2023.</li>
                <li id="dong2022survey">Dong, Qingxiu, Li, Lei, Dai, Damai, Zheng, Ce, Wu, Zhiyong, Chang, Baobao, Sun, Xu, Xu, Jingjing, and Sui, Zhifang. "A survey for in-context learning." <em>arXiv preprint arXiv:2301.00234</em>, 2023.</li>
                <li id="xie2021explanation">Xie, Sang Michael, Raghunathan, Aditi, Liang, Percy, and Ma, Tengyu. "An explanation of in-context learning as implicit bayesian inference." In: <em>International Conference on Learning Representations (ICLR)</em>, 2022.</li>
                <li id="garg2022can">Garg, Shivam, Tsipras, Dimitris, Liang, Percy S., and Valiant, Gregory. "What can transformers learn in-context? a case study of simple function classes." In: <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022.</li>
                <li id="von2023transformers">Von Oswald, Johannes, Niklasson, Eyvind, Randazzo, Ettore, Sacramento, João, Mordvintsev, Alexander, Zhmoginov, Andrey, and Vladymyrov, Max. "Transformers learn in-context by gradient descent." In: <em>International Conference on Machine Learning</em>, PMLR, 2023.</li>
                <li id="akyurek2022learning">Akyürek, Ekin, Schuurmans, Dale, Andreas, Jacob, Ma, Tengyu, and Zhou, Denny. "What learning algorithm is in-context learning? investigations with linear models." In: <em>International Conference on Learning Representations (ICLR)</em>, 2023.</li>
                <li id="mahankali2023one">Mahankali, Arvind, Hashimoto, Tatsunori B., and Ma, Tengyu. "One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention." <em>arXiv preprint arXiv:2307.03576</em>, 2023.</li>
                <li id="zhang2023trained">Zhang, Ruiqi, Frei, Spencer, and Bartlett, Peter L. "Trained Transformers Learn Linear Models In-Context." <em>arXiv preprint arXiv:2306.09927</em>, 2023.</li>
                <li id="tarzanagh2023transformers">Tarzanagh, Davoud Ataee, Li, Yingcong, Thrampoulidis, Christos, and Oymak, Samet. "Transformers as Support Vector Machines." <em>arXiv preprint arXiv:2308.16898</em>, 2023.</li>
                <li id="panigrahi2023trainable">Panigrahi, Abhishek, Malladi, Sadhika, Xia, Mengzhou, and Arora, Sanjeev. "Trainable transformer in transformer." <em>arXiv preprint arXiv:2307.01189</em>, 2023.</li>
                <li id="ahn2023transformers">Ahn, Kwangjun, Cheng, Xiang, Daneshmand, Hadi, and Sra, Suvrit. "Transformers learn to implement preconditioned gradient descent for in-context learning." <em>arXiv preprint arXiv:2306.00297</em>, 2023.</li>
                <li id="liu2022makes">Liu, Jiachang, Shen, Dinghan, Zhang, Yizhe, Dolan, Bill, Carin, Lawrence, and Chen, Weizhu. "What Makes Good In-Context Examples for GPT-3?" In: <em>Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</em>, 2022.</li>
                <li id="nagler2023statistical">Nagler, Thomas. "Statistical Foundations of Prior-Data Fitted Networks." In: <em>International Conference on Machine Learning (ICML)</em>, 2023.</li>
                <li id="liu2023pre">Liu, Pengfei, Yuan, Weizhe, Fu, Jinlan, Jiang, Zhengbao, Hayashi, Hiroaki, and Neubig, Graham. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." <em>ACM Computing Surveys</em>, vol. 55, ACM New York, NY, 2023.</li>
                <li id="cook1980characterizations">Cook, R Dennis and Weisberg, Sanford. "Characterizations of an empirical influence function for detecting influential cases in regression." <em>Technometrics</em>, vol. 22, no. 4, 1980, pp. 495–508, Taylor & Francis.</li>
                <li id="zhang2021rethinking">Zhang, Rui and Zhang, Shihua. "Rethinking Influence Functions of Neural Networks in the Over-parameterized Regime." In: <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2021.</li>
                <li id="guo2019certified">Guo, Chuan, Goldstein, Tom, Hannun, Awni, and Van Der Maaten, Laurens. "Certified data removal from machine learning models." In: <em>International Conference on Machine Learning (ICML)</em>, 2019.</li>
                <li id="huang2023tight">Huang, Yiyang and Canonne, Clément L. "Tight Bounds for Machine Unlearning via Differential Privacy." <em>arXiv preprint arXiv:2309.00886</em>, 2023.</li>
                <li id="sekhari2021remember">Sekhari, Ayush, Acharya, Jayadev, Kamath, Gautam, and Suresh, Ananda Theertha. "Remember What You Want to Forget: Algorithms for Machine Unlearning." In: <em>Advances in Neural Information Processing Systems</em>, 2021.</li>
                <li id="ghorbani2020distributional">Ghorbani, Amirata, Kim, Michael, and Zou, James. "A distributional framework for data valuation." In: <em>International Conference on Machine Learning (ICML)</em>, PMLR, 2020.</li>
                <li id="ghorbani19shapley">Ghorbani, Amirata and Zou, James. "Data Shapley: Equitable Valuation of Data for Machine Learning." In: <em>Proceedings of the 36th International Conference on Machine Learning (ICML)</em>, 2019.</li>
                <li id="wu2020deltagrad">Wu, Yinjun, Dobriban, Edgar, and Davidson, Susan. "DeltaGrad: Rapid retraining of machine learning models." In: <em>International Conference on Machine Learning (ICML)</em>, 2020.</li>
                <li id="izzo2021">Izzo, Zachary, Smart, Mary Anne, Chaudhuri, Kamalika, and Zou, James. "Approximate Data Deletion from Machine Learning Models." In: <em>Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2021.</li>
                <li id="Golatkar_2020_CVPR">Golatkar, Aditya, Achille, Alessandro, and Soatto, Stefano. "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks." In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</li>
                <li id="golatkar2020forget">Golatkar, Aditya, Achille, Alessandro, and Soatto, Stefano. "Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations." <em>arXiv preprint arXiv:2003.02960</em>, 2020.</li>
                <li id="neel2021deletion">Neel, Seth, Roth, Aaron, and Sharifi-Malvajerdi, Saeed. "Descent-to-Delete: Gradient-Based Methods for Machine Unlearning." In: <em>Proceedings of the 32nd International Conference on Algorithmic Learning Theory (ALT)</em>, 2021.</li>
                <li id="ginart2019making">Ginart, Antonio, Guan, Melody Y., Valiant, Gregory, and Zou, James. "Making AI Forget You: Data Deletion in Machine Learning." In: <em>Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</em>, 2019.</li>
                <li id="pawelczyk2022trade">Pawelczyk, Martin, Leemann, Tobias, Biega, Asia, and Kasneci, Gjergji. "On the Trade-Off between actionable explanations and the right to be forgotten." In: <em>International Conference on Learning Representations (ICLR)</em>, 2023.</li>
                <li id="rong2022evaluating">Rong, Yao, Leemann, Tobias, Borisov, Vadim, Kasneci, Gjergji, and Kasneci, Enkelejda. "A Consistent and Efficient Evaluation Strategy for Attribution Methods." In: <em>International Conference on Machine Learning (ICML)</em>, 2022.</li>
                <li id="hooker2019bench">Hooker, Sara, Erhan, Dumitru, Kindermans, Pieter-Jan, and Kim, Been. "A Benchmark for Interpretability Methods in Deep Neural Networks." In: <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.</li>
                <li id="doshi2017rigorous">Doshi-Velez, Finale and Kim, Been. "Towards A Rigorous Science of Interpretable Machine Learning." <em>arXiv preprint arXiv:1702.08608</em>, 2017.</li>
                <li id="jang2022knowledge">Jang, Joel, Yoon, Dongkeun, Yang, Sohee, Cha, Sungmin, Lee, Moontae, Logeswaran, Lajanugen, and Seo, Minjoon. "Knowledge Unlearning for Mitigating Privacy Risks in Language Models." In: <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em>, 2023.</li>
                <li id="goel2022towards">Goel, Shashwat, Prabhu, Ameya, Sanyal, Amartya, Lim, Ser-Nam, Torr, Philip, and Kumaraguru, Ponnurangam. "Towards adversarial evaluations for inexact machine unlearning." <em>arXiv preprint arXiv:2201.06640</em>, 2022.</li>
                <li id="kurmanji2023towards">Kurmanji, Meghdad, Triantafillou, Peter, and Triantafillou, Eleni. "Towards Unbounded Machine Unlearning." <em>arXiv preprint arXiv:2302.09880</em>, 2023.</li>
                <li id="shokri2017membership">Shokri, Reza, Stronati, Marco, Song, Congzheng, and Shmatikov, Vitaly. "Membership inference attacks against machine learning models." In: <em>2017 IEEE symposium on security and privacy (SP)</em>, IEEE, 2017, pp. 3–18.</li>
                <li id="carlini2021membership">Carlini, Nicholas, Chien, Steve, Nasr, Milad, Song, Shuang, Terzis, Andreas, and Tramer, Florian. "Membership inference attacks from first principles." In: <em>2022 IEEE Symposium on Security and Privacy (SP)</em>, IEEE, 2022, pp. 1897–1914.</li>
                <li id="leemann2023gaussian">Leemann, Tobias, Pawelczyk, Martin, and Kasneci, Gjergji. "Gaussian Membership Inference Privacy." In: <em>Advances in neural information processing systems (NeurIPS)</em>, 2023.</li>
                <li id="carlini2022quantifying">Carlini, Nicholas, Ippolito, Daphne, Jagielski, Matthew, Lee, Katherine, Tramer, Florian, and Zhang, Chiyuan. "Quantifying memorization across neural language models." In: <em>International Conference on Learning Representations (ICLR)</em>, 2023.</li>
                <li id="jagielski2022measuring">Jagielski, Matthew, Thakkar, Om, Tramer, Florian, Ippolito, Daphne, Lee, Katherine, Carlini, Nicholas, Wallace, Eric, Song, Shuang, Thakurta, Abhradeep, Papernot, Nicolas, et al. "Measuring forgetting of memorized training examples." In: <em>International Conference on Learning Representations (ICLR)</em>, 2023.</li>
                <li id="tirumala2022memorization">Tirumala, Kushal, Markosyan, Aram, Zettlemoyer, Luke, and Aghajanyan, Armen. "Memorization without overfitting: Analyzing the training dynamics of large language models." In: <em>Advances in Neural Information Processing Systems</em>, 2022.</li>
                <li id="graves2021amnesiac">Graves, Laura, Nagisetty, Vineel, and Ganesh, Vijay. "Amnesiac machine learning." In: <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2021.</li>
                <li id="neyman1933ix">Neyman, Jerzy and Pearson, Egon Sharpe. "IX. On the problem of the most efficient tests of statistical hypotheses." <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em>, vol. 231, no. 694-706, 1933, pp. 289–337, The Royal Society London.</li>
                <li id="goldsteen2021data">Goldsteen, Abigail, Ezov, Gilad, Shmelkin, Ron, Moffie, Micha, and Farkash, Ariel. "Data minimization for GDPR Compliance in machine learning models." <em>AI and Ethics</em>, 2021, Springer.</li>
                <li id="biega2020dm">Biega, Asia J., Potash, Peter, Daumé, Hal, Diaz, Fernando, and Finck, Michèle. "Operationalizing the Legal Principle of Data Minimization for Personalization." In: <em>Proceedings of the ACM(43) SIGIR '20</em>, 2020, pp. 399–408.</li>
                <li id="regulation2016regulation">European Union. "Regulation (EU) 2016/679 of the European Parliament and of the Council." <em>Official Journal of the European Union</em>, 2016.</li>
                <li id="ccpa2021">CA OAG. "CCPA regulations: Final regulation text." <em>Office of the Attorney General, California Department of Justice</em>, 2021.</li>
                <li id="voigt2017eu">Voigt, Paul and Von dem Bussche, Axel. "The eu general data protection regulation (gdpr)." In: <em>A Practical Guide, 1st Ed.</em>, Cham: Springer International Publishing, 2017.</li>
                <li id="thudi2022unroll">Thudi, Anvith, Deza, Gabriel, Chandrasekaran, Varun, and Papernot, Nicolas. "Unrolling SGD: Understanding Factors Influencing Machine Unlearning." In: <em>2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P)</em>, IEEE, 2022.</li>
                <li id="peste2021ssse">Peste, Alexandra, Alistarh, Dan, and Lampert, Christoph H. "Ssse: Efficiently erasing samples from trained machine learning models." <em>arXiv preprint arXiv:2107.03860</em>, 2021.</li>
                <li id="golatkar2021mixed">Golatkar, Aditya, Achille, Alessandro, Ravichandran, Avinash, Polito, Marzia, and Soatto, Stefano. "Mixed-privacy forgetting in deep networks." In: <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 792–801.</li>
                <li id="socher2013recursive">Socher, Richard, Perelygin, Alex, Wu, Jean, Chuang, Jason, Manning, Christopher D., Ng, Andrew, and Potts, Christopher. "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank." In: <em>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>, 2013.</li>
                <li id="zhang2015character">Zhang, Xiang, Zhao, Junbo, and LeCun, Yann. "Character-level convolutional networks for text classification." In: <em>Advances in neural information processing systems (NeurIPS)</em>, vol. 28, 2015.</li>
                <li id="maas2011learning">Maas, Andrew, Daly, Raymond E., Pham, Peter T., Huang, Dan, Ng, Andrew Y., and Potts, Christopher. "Learning word vectors for sentiment analysis." In: <em>Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</em>, 2011, pp. 142–150.</li>
                <li id="pang2005seeing">Pang, Bo and Lee, Lillian. "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales." <em>arXiv preprint cs/0506075</em>, 2005.</li>

                <li id="bourt">Bourtoule, Lucas, Chandrasekaran, Varun, Choquette-Choo, Christopher A., Jia, Hengrui, Travers, Adelin, Zhang, Baiwu, Lie, David, and Papernot, Nicolas. "Machine Unlearning." In: <em>42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021</em>, IEEE, 2021, pp. 141–159. <a href="https://doi.org/10.1109/SP40001.2021.00019" target="_blank">https://doi.org/10.1109/SP40001.2021.00019</a>.</li>
                <li id="tsyb">Tsybakov, Alexandre B. "Introduction to Nonparametric Estimation." Springer Publishing Company, 2008, ISBN: 0387790519.</li>
                <li id="touvron2023llama">Touvron, Hugo, Martin, Louis, Stone, Kevin, Albert, Peter, Almahairi, Amjad, Babaei, Yasmine, Bashlykov, Nikolay, Batra, Soumya, Bhargava, Prajjwal, Bhosale, Shruti, et al. "Llama 2: Open foundation and fine-tuned chat models." <em>arXiv preprint arXiv:2307.09288</em>, 2023.</li>
                <li id="biderman2023pythia">Biderman, Stella, Schoelkopf, Hailey, Anthony, Quentin Gregory, Bradley, Herbie, O’Brien, Kyle, Hallahan, Eric, Khan, Mohammad Aflah, Purohit, Shivanshu, Prashanth, USVSN Sai, Raff, Edward, et al. "Pythia: A suite for analyzing large language models across training and scaling." In: <em>International Conference on Machine Learning</em>, PMLR, 2023, pp. 2397–2430.</li>
                <li id="Gupta2021AdaptiveMU">Gupta, Varun, Jung, Christopher, Neel, Seth, Roth, Aaron, Sharifi-Malvajerdi, Saeed, and Waites, Chris. "Adaptive Machine Unlearning." <em>arXiv preprint arXiv:2106.04378</em>, 2021. <a href="https://api.semanticscholar.org/CorpusID:235367846" target="_blank">https://api.semanticscholar.org/CorpusID:235367846</a>.</li>
                <li id="wang2023kga">Wang, Lingzhi, Chen, Tong, Yuan, Wei, Zeng, Xingshan, Wong, Kam-Fai, and Yin, Hongzhi. "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment." <em>arXiv preprint arXiv:2305.06535</em>, 2023.</li>
                <li id="chien2024stochastic">Chien, Eli, Wang, Haoyu, Chen, Ziang, and Li, Pan. "Stochastic Gradient Langevin Unlearning." <em>arXiv preprint arXiv:2403.17105</em>, 2024.</li>
                <li id="ravfogel2022adversarial">Ravfogel, Shauli, Vargas, Francisco, Goldberg, Yoav, and Cotterell, Ryan. "Adversarial concept erasure in kernel space." In: <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 6034–6055.</li>
                <li id="ravfogel2022linear">Ravfogel, Shauli, Twiton, Michael, Goldberg, Yoav, and Cotterell, Ryan D. "Linear adversarial concept erasure." In: <em>International Conference on Machine Learning</em>, PMLR, 2022, pp. 18400–18421.</li>
                <li id="belrose2023leace">Belrose, Nora, Schneider-Joseph, David, Ravfogel, Shauli, Cotterell, Ryan, Raff, Edward, and Biderman, Stella. "LEACE: Perfect linear concept erasure in closed form." <em>arXiv preprint arXiv:2306.03819</em>, 2023.</li>
                <li id="datamodels">Ilyas, Andrew, Park, Sung Min, Engstrom, Logan, Leclerc, Guillaume, and Madry, Aleksander. "Datamodels: Predicting Predictions from Training Data." <em>arXiv preprint arXiv:2202.00622</em>, 2022. <a href="https://arxiv.org/abs/2202.00622" target="_blank">https://arxiv.org/abs/2202.00622</a>.</li>
                <li id="making_ai_forget">Ginart, Antonio, Guan, Melody Y., Valiant, Gregory, and Zou, James. "Making AI Forget You: Data Deletion in Machine Learning." <em>arXiv preprint arXiv:1907.05012</em>, 2019.</li>
                <li id="descent_to_delete">Neel, Seth, Roth, Aaron, and Sharifi-Malvajerdi, Saeed. "Descent-to-Delete: Gradient-Based Methods for Machine Unlearning." <em>arXiv preprint arXiv:2007.02923</em>, 2020.</li>
                <li id="incontextunlearning">Pawelczyk, Martin, Neel, Seth, and Lakkaraju, Himabindu. "In-Context Unlearning: Language Models as Few Shot Unlearners." <em>arXiv preprint arXiv:2310.07579</em>, 2023.</li>
                <li id="dual_random">Zhang, Lijun, Mahdavi, Mehrdad, Jin, Rong, and Yang, Tianbao. "Recovering Optimal Solution by Dual Random Projection." <em>arXiv preprint arXiv:1211.3046</em>, 2012. <a href="http://arxiv.org/abs/1211.3046" target="_blank">http://arxiv.org/abs/1211.3046</a>.</li>
                <li id="group_influence_approx">Koh, Pang Wei, Ang, Kai-Siang, Teo, Hubert H. K., and Liang, Percy. "On the Accuracy of Influence Functions for Measuring Group Effects." <em>arXiv preprint arXiv:1905.13289</em>, 2019.</li>
                <li id="approximate_data_deletion">Izzo, Zachary, Smart, Mary Anne, Chaudhuri, Kamalika, and Zou, James. "Approximate Data Deletion from Machine Learning Models." <em>arXiv preprint arXiv:2002.10077</em>, 2021.</li>
                <li id="hu2021lora">Hu, Edward J., Shen, Yelong, Wallis, Phillip, Allen-Zhu, Zeyuan, Li, Yuanzhi, Wang, Shean, Wang, Lu, and Chen, Weizhu. "Lora: Low-rank adaptation of large language models." <em>arXiv preprint arXiv:2106.09685</em>, 2021.</li>
                <li id="malladi2023kernel">Malladi, Sadhika, Wettig, Alexander, Yu, Dingli, Chen, Danqi, and Arora, Sanjeev. "A kernel-based view of language model fine-tuning." In: <em>International Conference on Machine Learning</em>, PMLR, 2023, pp. 23610–23641.</li>
                <li id="certified_data_removal">Guo, Chuan, Goldstein, Tom, Hannun, Awni, and van der Maaten, Laurens. "Certified Data Removal from Machine Learning Models." <em>arXiv preprint arXiv:1911.03030</em>, 2023.</li>
                <li id="certifiable_linear_unlearning">Mahadevan, Ananth and Mathioudakis, Michael. "Certifiable Machine Unlearning for Linear Models." <em>arXiv preprint arXiv:2106.15093</em>, 2021.</li>
                <li id="group_influence_functions">Koh, Pang Wei, Ang, Kai-Siang, Teo, Hubert H. K., and Liang, Percy. "On the Accuracy of Influence Functions for Measuring Group Effects." <em>arXiv preprint arXiv:1905.13289</em>, 2019.</li>
                <li id="TRAK">Park, Sung Min, Georgiev, Kristian, Ilyas, Andrew, Leclerc, Guillaume, and Madry, Aleksander. "TRAK: Attributing Model Behavior at Scale." <em>arXiv preprint arXiv:2303.14186</em>, 2023.</li>
                <li id="SGM_rdp">Mironov, Ilya, Talwar, Kunal, and Zhang, Li. "Rényi Differential Privacy of the Sampled Gaussian Mechanism." <em>arXiv preprint arXiv:1908.10530</em>, 2019. <a href="https://arxiv.org/abs/1908.10530" target="_blank">https://arxiv.org/abs/1908.10530</a>.</li>
                <li id="RDP_2017">Mironov, Ilya. "Rényi Differential Privacy." In: <em>2017 IEEE 30th Computer Security Foundations Symposium (CSF)</em>, IEEE, 2017. <a href="https://doi.org/10.1109%2Fcsf.2017.11" target="_blank">https://doi.org/10.1109%2Fcsf.2017.11</a>.</li>
                <li id="privacy_book">Dwork, Cynthia and Roth, Aaron. "The Algorithmic Foundations of Differential Privacy." Now Publishers Inc., 2014, ISBN: 1551-305X. <a href="https://doi.org/10.1561/0400000042" target="_blank">https://doi.org/10.1561/0400000042</a>.</li>
                <li id="pate_2017">Papernot, Nicolas, Abadi, Martín, Erlingsson, Úlfar, Goodfellow, Ian, and Talwar, Kunal. "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data." In: <em>Proceedings of the International Conference on Learning Representations</em>, 2017. <a href="https://arxiv.org/abs/1610.05755" target="_blank">https://arxiv.org/abs/1610.05755</a>.</li>
                <li id="pate_2018">Papernot, Nicolas, Song, Shuang, Mironov, Ilya, Raghunathan, Ananth, Talwar, Kunal, and Erlingsson, Úlfar. "Scalable Private Learning with PATE." In: <em>International Conference on Learning Representations (ICLR)</em>, 2018. <a href="https://arxiv.org/abs/1802.08908" target="_blank">https://arxiv.org/abs/1802.08908</a>.</li>
                <li id="dpsgd">Abadi, Martin, Chu, Andy, Goodfellow, Ian, McMahan, H. Brendan, Mironov, Ilya, Talwar, Kunal, and Zhang, Li. "Deep Learning with Differential Privacy." In: <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, ACM, 2016, pp. 2976749–2978318. <a href="https://doi.org/10.1145%2F2976749.2978318" target="_blank">https://doi.org/10.1145%2F2976749.2978318</a>.</li>
                <li id="PLRV_for_gaussian">Canonne, Clément L., Kamath, Gautam, and Steinke, Thomas. "The Discrete Gaussian for Differential Privacy." <em>arXiv preprint arXiv:2004.00010</em>, 2020. <a href="https://arxiv.org/abs/2004.00010" target="_blank">https://arxiv.org/abs/2004.00010</a>.</li>
                <li id="microsoft_PRV_2">Gopi, Sivakanth, Lee, Yin Tat, and Wutschitz, Lukas. "Numerical Composition of Differential Privacy." <em>arXiv preprint arXiv:2106.02848</em>, 2021. <a href="https://arxiv.org/abs/2106.02848" target="_blank">https://arxiv.org/abs/2106.02848</a>.</li>
                <li id="privacy_amplification_theorem_1">Kasiviswanathan, Shiva Prasad, Lee, Homin K., Nissim, Kobbi, Raskhodnikova, Sofya, and Smith, Adam. "What Can We Learn Privately?" <em>arXiv preprint arXiv:0803.0924</em>, 2010.</li>
                <li id="privacy_amplification_theorem_2">Beimel, Amos, Brenner, Hai, Kasiviswanathan, Shiva Prasad, and Nissim, Kobbi. "Bounds on the sample complexity for private learning and private data release." <em>Machine Learning</em>, vol. 94, no. 3, Springer, 2013, pp. 401–437. <a href="https://doi.org/10.1007/s10994-013-5404-1" target="_blank">https://doi.org/10.1007/s10994-013-5404-1</a>.</li>
                <li id="privacy_amplification_by_subsampling">Balle, Borja, Barthe, Gilles, and Gaboardi, Marco. "Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences." <em>arXiv preprint arXiv:1807.01647</em>, 2018.</li>
                <li id="individualized_PATE">Boenisch, Franziska, Mühl, Christopher, Rinberg, Roy, Ihrig, Jannis, and Dziedzic, Adam. "Individualized PATE: Differentially Private Machine Learning with Individual Privacy Guarantees." <em>arXiv preprint arXiv:2202.10517</em>, 2022.</li>
                <li id="opacus_paper">Yousefpour, Ashkan, Shilov, Igor, Sablayrolles, Alexandre, Testuggine, Davide, Prasad, Karthik, Malek, Mani, Nguyen, John, Ghosh, Sayan, Bharadwaj, Akash, Zhao, Jessica, Cormode, Graham, and Mironov, Ilya. "Opacus: User-Friendly Differential Privacy Library in PyTorch." In: <em>NeurIPS 2021 Workshop Privacy in Machine Learning</em>, 2021. <a href="https://openreview.net/forum?id=EopKEYBoI-" target="_blank">https://openreview.net/forum?id=EopKEYBoI-</a>.</li>
                <li id="hyperparameter_tuning">Papernot, Nicolas and Steinke, Thomas. "Hyperparameter Tuning with Renyi Differential Privacy." <em>arXiv preprint arXiv:2110.03620</em>, 2022.</li>
                <li id="li2024wmdpbenchmarkmeasuringreducing">Li, Nathaniel, Pan, Alexander, Gopal, Anjali, Yue, Summer, Berrios, Daniel, Gatti, Alice, Li, Justin D., Dombrowski, Ann-Kathrin, Goel, Shashwat, Phan, Long, Mukobi, Gabriel, Helm-Burger, Nathan, Lababidi, Rassin, Justen, Lennart, Liu, Andrew B., Chen, Michael, Barrass, Isabelle, Zhang, Oliver, Zhu, Xiaoyuan, Tamirisa, Rishub, Bharathi, Bhrugu, Khoja, Adam, Zhao, Zhenqi, Herbert-Voss, Ariel, Breuer, Cort B., Marks, Samuel, Patel, Oam, Zou, Andy, Mazeika, Mantas, Wang, Zifan, Oswal, Palash, Lin, Weiran, Hunt, Adam A., Tienken-Harder, Justin, Shih, Kevin Y., Talley, Kemper, Guan, John, Steneker, Ian, Campbell, David, Jokubaitis, Brad, Levinson, Alex, Wang, Jean, Qian, William, Karmakar, Kallol Krishna, Basart, Steven, Fitz, Stephen, Levine, Mindy, Kumaraguru, Ponnurangam, Tupakula, Uday, Varadharajan, Vijay, Wang, Ruoyu, Shoshitaishvili, Yan, Ba, Jimmy, and Esvelt, Kevin M. "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning." <em>arXiv preprint arXiv:2403.03218</em>, 2024. <a href="https://arxiv.org/abs/2403.03218" target="_blank">https://arxiv.org/abs/2403.03218</a>.</li>
                <li id="max_causal_ent">Gleave, Adam and Toyer, Sam. "A Primer on Maximum Causal Entropy Inverse." <em>arXiv preprint arXiv:2203.11409</em>, 2022. <a href="https://arxiv.org/abs/2203.11409" target="_blank">https://arxiv.org/abs/2203.11409</a>.</li>
                <li id="max_ent">Ziebart, Brian D., Maas, Andrew, Bagnell, J. Andrew, and Dey, Anind K. "Maximum Entropy Inverse Reinforcement Learning." In: <em>Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</em>, 2008. <a href="https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf" target="_blank">https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf</a>.</li>
                <li id="goel2024correctivemachineunlearning">Goel, Shashwat, Prabhu, Ameya, Torr, Philip, Kumaraguru, Ponnurangam, and Sanyal, Amartya. "Corrective Machine Unlearning." <em>arXiv preprint arXiv:2402.14015</em>, 2024. <a href="https://arxiv.org/abs/2402.14015" target="_blank">https://arxiv.org/abs/2402.14015</a>.</li>
                <li id="membership_inference_attack">Gomrokchi, Maziar, Amin, Susan, Aboutalebi, Hossein, Wong, Alexander, and Precup, Doina. "Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning." <em>arXiv preprint arXiv:2109.03975</em>, 2022. <a href="https://arxiv.org/pdf/2109.03975.pdf" target="_blank">https://arxiv.org/pdf/2109.03975.pdf</a>.</li>
                <li id="irl_effective_medical">Yu, Chao, Liu, Jiming, and Zhao, Hongyi. "Inverse reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units." In: <em>BMC Medical Informatics and Decision Making</em>, Proceedings from the 4th China Health Information Processing Conference (CHIP 2018), vol. 19, no. 2, 2019. <a href="https://arxiv.org/pdf/2109.03975.pdf" target="_blank">https://arxiv.org/pdf/2109.03975.pdf</a>.</li>
                <li id="irl_introduction">Ng, Andrew Y. and Russell, Stuart. "Algorithms for Inverse Reinforcement Learning." In: <em>Proceedings of the Seventeenth International Conference on Machine Learning</em>, 2000, pp. 663–670. <a href="https://dl.acm.org/doi/10.5555/645529.657801" target="_blank">https://dl.acm.org/doi/10.5555/645529.657801</a>.</li>
                <li id="dp_membership_attack">Rahman, Md Atiqur, Rahman, Tanzila, Laganiere, Robert, Mohammed, Noman, and Wang, Yang. "Membership Inference Attack against Differentially Private Deep Learning Model." In: <em>Transactions on Data Privacy</em>, vol. 11, 2018, pp. 61–79. <a href="https://www.tdp.cat/issues16/tdp.a289a17.pdf" target="_blank">https://www.tdp.cat/issues16/tdp.a289a17.pdf</a>.</li>
                <li id="mimic_preprocessing">Gottesman, Omer, Futoma, Joseph, Liu, Yao, Parbhoo, Sonali, Celi, Leo, Brunskill, Emma, and Doshi-Velez, Finale. "Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions." In: <em>Proceedings of the 37th International Conference on Machine Learning</em>, vol. 119, 2020, pp. 3658–3667. <a href="https://proceedings.mlr.press/v119/gottesman20a.html" target="_blank">https://proceedings.mlr.press/v119/gottesman20a.html</a>.</li>
                <li id="private_k_means">Su, Dong, Cao, Jianneng, Li, Ninghui, Bertino, Elisa, and Jin, Hongxia. "Differentially Private k-Means Clustering." <em>arXiv preprint arXiv:1504.05998</em>, 2015.</li>
                <li id="curse_dimensionality">Bun, Mark, Ullman, Jonathan, and Vadhan, Salil. "Fingerprinting Codes and the Price of Approximate Differential Privacy." In: <em>SIAM Journal on Computing</em>, vol. 47, no. 5, 2018, pp. 1888–1938. <a href="https://scholar.harvard.edu/files/salil/files/stoc14.pdf" target="_blank">https://scholar.harvard.edu/files/salil/files/stoc14.pdf</a>.</li>
                <li id="lira">Carlini, Nicholas, Chien, Steve, Nasr, Milad, Song, Shuang, Terzis, Andreas, and Tramer, Florian. "Membership Inference Attacks From First Principles." <em>arXiv preprint arXiv:2112.03570</em>, 2022.</li>
                <li id="coinpress">Biswas, Sourav, Dong, Yihe, Kamath, Gautam, and Ullman, Jonathan. "CoinPress: Practical Private Mean and Covariance Estimation." <em>arXiv preprint arXiv:2006.06618</em>, 2022.</li>
                <li id="mnist">Deng, Li. "The mnist database of handwritten digit images for machine learning research." <em>IEEE Signal Processing Magazine</em>, vol. 29, no. 6, 2012, pp. 141–142, IEEE.</li>
                <li id="advanced_composition">Kairouz, Peter, Oh, Sewoong, and Viswanath, Pramod. "The Composition Theorem for Differential Privacy." <em>arXiv preprint arXiv:1311.0776</em>, 2015.</li>
                <li id="privacy_amplification_by_subsampling">Balle, Borja, Barthe, Gilles, and Gaboardi, Marco. "Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences." <em>arXiv preprint arXiv:1807.01647</em>, 2018.</li>
                                
            </ol>
        </section> 
        
    </main>

    <footer>
        <p>&copy; 2024 [ Georgiev, Park, Rinberg, Garg, Ilyas, Aleksander Madry, Neel]. All rights reserved.</p>
    </footer>
</body>
</html>
